---
title: "Assignment 3"
author: "202274326"
date: "AT 2023"
output: html_document
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(echo = FALSE) 
```

## Exercise 1
Here I will create a new SQLite database in my database folder, and then check if it exists.

```{r exercise 1, echo=TRUE}

#Load the packages.
library(DBI)
library(RSQLite)

#Create a new databse in my databse folder.
dir.create("database")
new_folder <- file.path(getwd(), "database")
db_path <- file.path(new_folder, "my_database.sqlite")
con <- DBI::dbConnect(RSQLite::SQLite(), dbname = db_path)
my_db <- "my_database.sqlite"

#Check for the existence of my relational database.
setwd("database")
if (file.exists(my_db)) {
  cat("The database file exists!\n")
} else {
  cat("The database file does not exist.\n")
}

```
## Exercise 2 - a. Gathering structured data
Here I will write a automatic function to extract two tables from a Wikipedia page. 

```{r exercise 2, echo = FALSE}

# Load the packages.
library(RSelenium)
library(tidyverse)
library(rvest)
library(xml2)
library(netstat)

# Set up the driver and navigate to the target url.
rD <- rsDriver(browser=c("firefox"), verbose = F, port = 2345L, chromever = NULL) 
driver <- rD[["client"]]
url <- 'https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States'
driver$navigate(url)

# Write a function to get tables and urls automatically.
table_scrape <- function() {
  src <- driver$getPageSource() # Get source code of the page
  
  # Get all the tables' html on the Wikipedia page.
  result_html <- read_html(src[[1]]) %>%
    html_elements("table")
  
  # Scrape all the tables.
  result_table <- result_html %>%
    html_table()
  
  # Remove the fifth column of the second table (the R2 table), because there are no values inside.
  result_table[[2]] <- result_table[[2]] %>% select(-5)

  # Create a blank list to store the final table results.
  extracted_table <- list()
  
  # Use a for loop to traverse the first two tables (the R1 and R2) to extract the relevant URL of each institution.
  # Then combine the URL columns into the original tables, and store them respectively into the list created before.
  for (i in c(1,2)) {
    links <- result_html[[i]]%>%
      html_nodes("td:nth-child(1) a") %>%
      html_attr("href")
    links_df <- data.frame(Wikipedia_Link = links)
    new_table <- bind_cols(result_table[[i]], links_df)
    extracted_table[[i]] <- new_table
  }
  
  # Now the item "extracted_table" contains the R1 and R2 tables we need. Here I will rename each table to make them more explicit.
  names(extracted_table) <- c("R1 (Very High Research Activity)", "R2 (High Research Activity)")
  
  # Return the function result.
  return(extracted_table)
}

# Scrape the tables.
structured_tables <- table_scrape()
print(structured_tables)

# Close the browser. 
driver$close()


```

```{r echo_example, echo=TRUE}
# {[language] [chunk_name], [chunk_options]}
# here we use echo=TRUE to override our global options and make the chunk appear exactly here. 

print("This code chunk is visible in this section.")
```

## Appendix: All code in this assignment

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 
# this chunk generates the complete code appendix. 
# eval=FALSE tells R not to run (``evaluate'') the code here (it was already run before).
```
